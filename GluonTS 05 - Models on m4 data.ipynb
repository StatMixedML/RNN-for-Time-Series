{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark m4\n",
    "\n",
    "How to fit several model and evaluate its predictions on three of subsets of M4 data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functools.partial\n",
    "\n",
    "Return a new `partial` object which when called will behave like *func* called with the positional arguments args and keyword arguments keywords. \n",
    "\n",
    "- [Source](https://docs.python.org/2/library/functools.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods  \n",
    "\n",
    "Brief overview of the used methods\n",
    "\n",
    "### `DeepAREstimator`\n",
    "\n",
    "- [`gluonts.model.deepar`](https://gluon-ts.mxnet.io/api/gluonts/gluonts.model.deepar.html)\n",
    "\n",
    "Construct a DeepAR estimator, implements an RNN-based model, close to the one described in\n",
    "\n",
    "- Salinas, David, Valentin Flunkert, and Jan Gasthaus. “DeepAR: Probabilistic forecasting with autoregressive recurrent networks.” arXiv preprint arXiv:1704.04110 (2017).\n",
    "\n",
    "\n",
    "### `MQCNNEstimator`\n",
    "\n",
    "- [`gluonts.model.seq2seq`](http://gluon-ts.mxnet.io/api/gluonts/gluonts.model.seq2seq.html)\n",
    "\n",
    "An `MQDNNEstimator` with Convolutional Neural Network (CNN) as an encoder. Implements the MQ-CNN Forecaster, proposed in \n",
    "\n",
    "- Wen, Ruofeng, et al. “A multi-horizon quantile recurrent forecaster.” arXiv preprint arXiv:1711.11053 (2017).\n",
    "\n",
    "\n",
    "### `SimpleFeedForwardEstimator`\n",
    "\n",
    "- [`gluonts.model.simple_feedforward`](http://gluon-ts.mxnet.io/api/gluonts/gluonts.model.simple_feedforward.html)\n",
    "\n",
    "A simple multilayer perceptron model predicting the next target time-steps given the previous ones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/awslabs/gluon-ts.git\n",
      "  Cloning https://github.com/awslabs/gluon-ts.git to c:\\users\\tm\\appdata\\local\\temp\\pip-req-build-_qy5pd3c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/awslabs/gluon-ts.git 'C:\\Users\\TM\\AppData\\Local\\Temp\\pip-req-build-_qy5pd3c'\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\TM\\Anaconda3\\python.exe' 'C:\\Users\\TM\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' get_requires_for_build_wheel 'C:\\Users\\TM\\AppData\\Local\\Temp\\tmpyfhvh3gn'\n",
      "       cwd: C:\\Users\\TM\\AppData\\Local\\Temp\\pip-req-build-_qy5pd3c\n",
      "  Complete output (18 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\TM\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py\", line 207, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\TM\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py\", line 197, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "    File \"C:\\Users\\TM\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py\", line 54, in get_requires_for_build_wheel\n",
      "      return hook(config_settings)\n",
      "    File \"C:\\Users\\TM\\AppData\\Local\\Temp\\pip-build-env-6xqtotbp\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 146, in get_requires_for_build_wheel\n",
      "      return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "    File \"C:\\Users\\TM\\AppData\\Local\\Temp\\pip-build-env-6xqtotbp\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 127, in _get_build_requires\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\TM\\AppData\\Local\\Temp\\pip-build-env-6xqtotbp\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 237, in run_setup\n",
      "      self).run_setup(setup_script=setup_script)\n",
      "    File \"C:\\Users\\TM\\AppData\\Local\\Temp\\pip-build-env-6xqtotbp\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 142, in run_setup\n",
      "      exec(compile(code, __file__, 'exec'), locals())\n",
      "    File \"setup.py\", line 22, in <module>\n",
      "      stdout=open(\"/dev/null\", \"w\"),\n",
      "  FileNotFoundError: [Errno 2] No such file or directory: '/dev/null'\n",
      "  ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\TM\\Anaconda3\\python.exe' 'C:\\Users\\TM\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' get_requires_for_build_wheel 'C:\\Users\\TM\\AppData\\Local\\Temp\\tmpyfhvh3gn' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "# install gluonts (most recent version)\n",
    "!pip install git+https://github.com/awslabs/gluon-ts.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "\n",
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "# gluonts imports\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.distribution.piecewise_linear import PiecewiseLinearOutput\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.trainer import Trainer\n",
    "\n",
    "# gluonts models\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.seq2seq import MQCNNEstimator\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.3\n"
     ]
    }
   ],
   "source": [
    "# gluonts version (important)\n",
    "import gluonts\n",
    "print(gluonts.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of series in m4 data: \n",
    "\n",
    "- Yearly: 23,000\n",
    "- Quarterly: 24,000\n",
    "- Monthly: 48,000\n",
    "- Weekly: 359\n",
    "- Daily: 4227\n",
    "- Hourly: 414\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m4_yearly']\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    \"m4_yearly\",\n",
    "]\n",
    "\n",
    "print(datasets)\n",
    "\n",
    "epochs = 1\n",
    "num_batches_per_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    partial(\n",
    "        SimpleFeedForwardEstimator,\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "    partial(\n",
    "        DeepAREstimator,\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "    partial(\n",
    "        DeepAREstimator,\n",
    "        distr_output=PiecewiseLinearOutput(8),\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "    partial(\n",
    "        MQCNNEstimator,\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_name, estimator):\n",
    "    dataset = get_dataset(dataset_name)\n",
    "    estimator = estimator(\n",
    "        prediction_length=dataset.metadata.prediction_length,\n",
    "        freq=dataset.metadata.freq,\n",
    "        use_feat_static_cat=True,\n",
    "        cardinality=[\n",
    "            feat_static_cat.cardinality\n",
    "            for feat_static_cat in dataset.metadata.feat_static_cat\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(f\"evaluating {estimator} on {dataset}\")\n",
    "\n",
    "    predictor = estimator.train(dataset.train)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset.test, predictor=predictor, num_eval_samples=100\n",
    "    )\n",
    "\n",
    "    agg_metrics, item_metrics = Evaluator()(\n",
    "        ts_it, forecast_it, num_series=len(dataset.test)\n",
    "    )\n",
    "\n",
    "    pprint.pprint(agg_metrics)\n",
    "\n",
    "    eval_dict = agg_metrics\n",
    "    eval_dict[\"dataset\"] = dataset_name\n",
    "    eval_dict[\"estimator\"] = type(estimator).__name__\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<class 'gluonts.model.simple_feedforward._estimator.SimpleFeedForwardEstimator'>, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=1, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=50, patience=10, weight_decay=1e-08))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using dataset already processed in path C:\\Users\\TM\\.mxnet\\gluon-ts\\datasets\\m4_yearly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() got an unexpected keyword argument 'use_feat_static_cat'\n",
      "functools.partial(<class 'gluonts.model.deepar._estimator.DeepAREstimator'>, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=1, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=50, patience=10, weight_decay=1e-08))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using dataset already processed in path C:\\Users\\TM\\.mxnet\\gluon-ts\\datasets\\m4_yearly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=1, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=50, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x000001F272B0EEB8>, test=<gluonts.dataset.common.FileDataset object at 0x000001F269C15780>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start model training\n",
      "INFO:root:Number of parameters in DeepARTrainingNetwork: 473443\n",
      "INFO:root:Epoch[0] Learning rate is 0.001\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.28it/s, avg_epoch_loss=8.59]\n",
      "INFO:root:Epoch[0] Elapsed time 2.632 seconds\n",
      "INFO:root:Epoch[0] Evaluation metric 'epoch_loss'=8.586752\n",
      "INFO:root:Loading parameters from best epoch (0)\n",
      "INFO:root:Final loss: 8.586752185821533 (occurred at epoch 0)\n",
      "INFO:root:End model training\n",
      "Running evaluation:   0%|          | 0/23000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot extract prediction target since the index of forecast is outside the index of target\n",
      "Index of forecast: DatetimeIndex(['1752-07-31', '1753-07-31', '1754-07-31', '1755-07-31',\n",
      "               '1756-07-31', '1757-07-31'],\n",
      "              dtype='datetime64[ns]', freq='12M')\n",
      " Index of target: DatetimeIndex(['1749-12-31', '1750-12-31', '1751-12-31', '1752-12-31',\n",
      "               '1753-12-31', '1754-12-31', '1755-12-31', '1756-12-31',\n",
      "               '1757-12-31', '1758-12-31', '1759-12-31', '1760-12-31',\n",
      "               '1761-12-31', '1762-12-31', '1763-12-31', '1764-12-31',\n",
      "               '1765-12-31', '1766-12-31', '1767-12-31', '1768-12-31',\n",
      "               '1769-12-31', '1770-12-31', '1771-12-31', '1772-12-31',\n",
      "               '1773-12-31', '1774-12-31', '1775-12-31', '1776-12-31',\n",
      "               '1777-12-31', '1778-12-31', '1779-12-31', '1780-12-31',\n",
      "               '1781-12-31', '1782-12-31', '1783-12-31', '1784-12-31',\n",
      "               '1785-12-31'],\n",
      "              dtype='datetime64[ns]', freq='12M')\n",
      "functools.partial(<class 'gluonts.model.deepar._estimator.DeepAREstimator'>, distr_output=gluonts.distribution.piecewise_linear.PiecewiseLinearOutput(num_pieces=8), trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=1, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=50, patience=10, weight_decay=1e-08))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using dataset already processed in path C:\\Users\\TM\\.mxnet\\gluon-ts\\datasets\\m4_yearly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.piecewise_linear.PiecewiseLinearOutput(num_pieces=8), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=1, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=50, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x000001F272A439B0>, test=<gluonts.dataset.common.FileDataset object at 0x000001F273490978>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start model training\n",
      "INFO:root:Number of parameters in DeepARTrainingNetwork: 473457\n",
      "INFO:root:Epoch[0] Learning rate is 0.001\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.93it/s, avg_epoch_loss=495]\n",
      "INFO:root:Epoch[0] Elapsed time 2.288 seconds\n",
      "INFO:root:Epoch[0] Evaluation metric 'epoch_loss'=495.098392\n",
      "INFO:root:Loading parameters from best epoch (0)\n",
      "INFO:root:Final loss: 495.09839233398435 (occurred at epoch 0)\n",
      "INFO:root:End model training\n",
      "Running evaluation:   0%|          | 0/23000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot extract prediction target since the index of forecast is outside the index of target\n",
      "Index of forecast: DatetimeIndex(['1752-07-31', '1753-07-31', '1754-07-31', '1755-07-31',\n",
      "               '1756-07-31', '1757-07-31'],\n",
      "              dtype='datetime64[ns]', freq='12M')\n",
      " Index of target: DatetimeIndex(['1749-12-31', '1750-12-31', '1751-12-31', '1752-12-31',\n",
      "               '1753-12-31', '1754-12-31', '1755-12-31', '1756-12-31',\n",
      "               '1757-12-31', '1758-12-31', '1759-12-31', '1760-12-31',\n",
      "               '1761-12-31', '1762-12-31', '1763-12-31', '1764-12-31',\n",
      "               '1765-12-31', '1766-12-31', '1767-12-31', '1768-12-31',\n",
      "               '1769-12-31', '1770-12-31', '1771-12-31', '1772-12-31',\n",
      "               '1773-12-31', '1774-12-31', '1775-12-31', '1776-12-31',\n",
      "               '1777-12-31', '1778-12-31', '1779-12-31', '1780-12-31',\n",
      "               '1781-12-31', '1782-12-31', '1783-12-31', '1784-12-31',\n",
      "               '1785-12-31'],\n",
      "              dtype='datetime64[ns]', freq='12M')\n",
      "functools.partial(<class 'gluonts.model.seq2seq._mq_dnn_estimator.MQCNNEstimator'>, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=1, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=50, patience=10, weight_decay=1e-08))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using dataset already processed in path C:\\Users\\TM\\.mxnet\\gluon-ts\\datasets\\m4_yearly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() got an unexpected keyword argument 'use_feat_static_cat'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e1ba79ac7724>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;34m\"MASE\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;34m\"sMAPE\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;34m\"MSIS\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         ]\n\u001b[0;32m     25\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2984\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2986\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 raise KeyError(\n\u001b[0;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1177\u001b[1;33m                         \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m                     )\n\u001b[0;32m   1179\u001b[0m                 )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    results = []\n",
    "    for dataset_name in datasets:\n",
    "        for estimator in estimators:\n",
    "            print(estimator)\n",
    "            # catch exceptions that are happening during training to avoid failing the whole evaluation\n",
    "            try:\n",
    "                results.append(evaluate(dataset_name, estimator))\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    sub_df = df[\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"estimator\",\n",
    "            \"RMSE\",\n",
    "            \"mean_wQuantileLoss\",\n",
    "            \"MASE\",\n",
    "            \"sMAPE\",\n",
    "            \"MSIS\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    print(sub_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"MASE\", \"sMAPE\", \"dataset\", \"estimator\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
