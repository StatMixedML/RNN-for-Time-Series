{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "print(mx.__version__)\n",
    "\n",
    "import gluonts\n",
    "print(gluonts.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n",
      "INFO:root:Using CPU\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.distribution.piecewise_linear import PiecewiseLinearOutput\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.seq2seq import MQCNNEstimator\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using CPU\n",
      "INFO:root:using dataset already processed in path C:\\Users\\TM\\.mxnet\\gluon-ts\\datasets\\m4_weekly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[359], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=None, freq=\"W\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=13, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=1, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=100, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True, use_feat_static_real=False) on TrainDatasets(metadata=<MetaData freq='W' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='359'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=13>, train=<gluonts.dataset.common.FileDataset object at 0x0000022BA6D5BDD8>, test=<gluonts.dataset.common.FileDataset object at 0x0000022BA6D5B7F0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start model training\n",
      "INFO:root:Epoch[0] Learning rate is 0.001\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]INFO:root:Number of parameters in DeepARTrainingNetwork: 49593\n",
      "  0%|          | 0/100 [00:03<?, ?it/s, avg_epoch_loss=9.09]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ef981ab2ecbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# catch exceptions that are happening during training to avoid failing the whole evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-ef981ab2ecbf>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(dataset_name, estimator)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"evaluating {estimator} on {dataset}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mpredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     forecast_it, ts_it = make_evaluation_predictions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gluonts\\model\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, training_data)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gluonts\\model\\estimator.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, training_data)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrained_net\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0minput_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_hybrid_forward_input_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mtrain_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         )\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gluonts\\trainer\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, net, input_names, train_iter)\u001b[0m\n\u001b[0;32m    258\u001b[0m                             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m                             \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m                             it.set_postfix(\n\u001b[0;32m    262\u001b[0m                                 ordered_dict={\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\metric.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, _, preds)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum_metric\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_inst\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1978\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1979\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1980\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1981\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "#     \"m4_hourly\",\n",
    "#     \"m4_daily\",\n",
    "#     \"m4_weekly\",\n",
    "#     \"m4_monthly\",\n",
    "    \"m4_quarterly\",\n",
    "    \"m4_yearly\",\n",
    "]\n",
    "\n",
    "epochs = 1\n",
    "num_batches_per_epoch = 100\n",
    "\n",
    "estimators = [\n",
    "    partial(\n",
    "        DeepAREstimator,\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch,\n",
    "#             ctx=\"gpu\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate(dataset_name, estimator):\n",
    "    dataset = get_dataset(dataset_name)\n",
    "    estimator = estimator(\n",
    "        prediction_length=dataset.metadata.prediction_length,\n",
    "        freq=dataset.metadata.freq,\n",
    "        use_feat_static_cat=True,\n",
    "        cardinality=[\n",
    "            feat_static_cat.cardinality\n",
    "            for feat_static_cat in dataset.metadata.feat_static_cat\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(f\"evaluating {estimator} on {dataset}\")\n",
    "\n",
    "    predictor = estimator.train(dataset.train)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset.test, predictor=predictor, num_samples=100\n",
    "    )\n",
    "\n",
    "    agg_metrics, item_metrics = Evaluator()(\n",
    "        ts_it, forecast_it, num_series=len(dataset.test)\n",
    "    )\n",
    "\n",
    "    pprint.pprint(agg_metrics)\n",
    "\n",
    "    eval_dict = agg_metrics\n",
    "    eval_dict[\"dataset\"] = dataset_name\n",
    "    eval_dict[\"estimator\"] = type(estimator).__name__\n",
    "    return eval_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    results = []\n",
    "    for dataset_name in datasets:\n",
    "        for estimator in estimators:\n",
    "            # catch exceptions that are happening during training to avoid failing the whole evaluation\n",
    "            try:\n",
    "                results.append(evaluate(dataset_name, estimator))\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    sub_df = df[\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"estimator\",\n",
    "            \"RMSE\",\n",
    "            \"mean_wQuantileLoss\",\n",
    "            \"MASE\",\n",
    "            \"sMAPE\",\n",
    "            \"MSIS\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    print(sub_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
